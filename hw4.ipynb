{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/nusanrana/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from audioop import reverse\n",
    "from queue import PriorityQueue\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "1) Build a bigram LM using the following two steps:\n",
    "\n",
    "1A) Use nltk to compile all the unique bigrams from the corpus you used for the previous assignment.  \n",
    "\n",
    "1B) Compute probability of each bigram using MLE (count(w1 w2)/count(w1)) \n",
    "\n",
    "2) Predict next word using the following steps:\n",
    "\n",
    "2A) Get an input word from user, inpW.\n",
    "\n",
    "2B) Use the bigram LM built in step 1 to find all the bigrams where the input word, inpW, is w1. \n",
    "Display all possible next words from these bigrams and their corresponding probabilities.  (Sort in descending order on probabilities)\"\"\"\n",
    "\n",
    "class bigram:\n",
    "\n",
    "    #function to normalize and tokenize corpus\n",
    "    #takes nltk corpus as param, returns list of words\n",
    "    def corpus_tokenize(valuec):\n",
    "        corpus = ' '.join(valuec)\n",
    "        normalize_corpus = corpus.lower()\n",
    "        tokenize = word_tokenize(normalize_corpus)\n",
    "        return tokenize\n",
    "  \n",
    "    #counting frequency of all words in the corpus\n",
    "    def vocab_builder(corpus):\n",
    "        vocab = {}\n",
    "        for x in corpus:\n",
    "            if vocab.get(x) == None:\n",
    "                vocab[x] = 1\n",
    "            else:\n",
    "                vocab[x] = vocab.get(x)+1\n",
    "        #print(vocab)\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    \n",
    "    #finding frequency of each bigram\n",
    "    def bigram_freq(bigrams):\n",
    "        bigram_freq  = {}\n",
    "        for x in bigrams:\n",
    "            if bigram_freq.get(x) == None:\n",
    "                    bigram_freq[x] = 1\n",
    "            else:\n",
    "                    bigram_freq[x] = bigram_freq.get(x)+1\n",
    "            #print(vocab)\n",
    "        return bigram_freq\n",
    "    \n",
    "    #1B) Compute probability of each bigram using MLE (count(w1 w2)/count(w1)) \n",
    "    def bigram_prob(bigram_freq, vocab):\n",
    "        bigram_prob = {}\n",
    "        for x in bigram_freq:\n",
    "            bigram_prob[x] = bigram_freq.get(x)/vocab.get(x[0])\n",
    "            \n",
    "        return bigram_prob\n",
    "    \n",
    "    #function to sort words by its probablity in value\n",
    "    #take possible word: prob dict as param, returns sorted dict in descendign order\n",
    "    def sorted_prob(prob_words):\n",
    "        sorted_distance = dict(sorted(prob_words.items(), key=lambda item: item[1],reverse=True)) #sorting words by probablity\n",
    "        return sorted_distance\n",
    "    \n",
    "    \n",
    "    #2A) Get an input word from user, inpW.\n",
    "    #takes dict of bigram and prob as param, returns dict of next word and prob\n",
    "    def user_input(bigram_probablity):\n",
    "        inpW = input(\"Enter a word: \")\n",
    "        user_word = inpW.lower()\n",
    "        next_word = {}\n",
    "        while user_word != 'q':\n",
    "            for x in bigram_probablity:\n",
    "                if user_word == x[0]:\n",
    "                    next_word[x[1]] = bigram_probablity.get(x)\n",
    "            inpW = input(\"Enter a word: \")\n",
    "            user_word = inpW.lower()\n",
    "\n",
    "\n",
    "        return next_word\n",
    "    \n",
    "    \n",
    "def run():  \n",
    "    \n",
    "    #creating bigram class instance\n",
    "    bigram_obj = bigram\n",
    "    \n",
    "    inaugural_corpus = inaugural.words('1865-Lincoln.txt') #using inaugural corpus\n",
    "    tokened_corpus = bigram_obj.corpus_tokenize(inaugural_corpus) #tokenizing corpus\n",
    "    \n",
    "    #creating a dict that holds word and it's frequency\n",
    "    vocab_freq =  bigram_obj.vocab_builder(tokened_corpus)\n",
    "\n",
    "    bigram_list = list((bigrams(tokened_corpus))) #list of all the bigrams in the corpus_tokenize\n",
    "\n",
    "    #creating a dict that holds bigram and its frquency\n",
    "    bigram_freq =  bigram_obj.bigram_freq(bigram_list)\n",
    "    \n",
    "    #bigram probablity\n",
    "    bigram_prob = bigram_obj.bigram_prob(bigram_freq, vocab_freq)\n",
    "    \n",
    "    user_word = bigram_obj.user_input(bigram_prob)\n",
    "    sorted_prob = bigram_obj.sorted_prob(user_word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Total words in corpus:\", len(inaugural_corpus))\n",
    "    print(\"Unique vocab #: \",len(vocab_freq))\n",
    "    print(\"Total bigrams: \", len(bigram_list))\n",
    "    print(\"Unique bigram #: \",len(bigram_freq))\n",
    "    \n",
    "    #for x in range(10):\n",
    "    print(user_word)\n",
    "    print(\"\\nSored Prob\",list(sorted_prob))\n",
    "    #print( vocab_freq)\n",
    "    #print(bigram_freq)\n",
    "    print(\"\\nBigram: Prob: \",bigram_prob)\n",
    "    \n",
    "    #{'red': [3,1], 'blue': [2,2]}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36713bed272a9d114316a294fb6c1a826f423acbc1d9527c9480bc3366607df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
