{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/nusanrana/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter \"q\" to end: \n",
      "Courpus: 1865-Lincoln.txt from inaugural corpus\n",
      "Total words in corpus: 785\n",
      "Unique vocab #:  335\n",
      "Total bigrams:  704\n",
      "Unique bigram #:  648\n",
      "\n",
      "\n",
      "\n",
      "Possible next words and probablity after \"new\": \n",
      "Word: could     Probablity: 1.0\n",
      "\n",
      "\"half\" is not in corpus..\n",
      "\n",
      "Possible next words and probablity after \"the\": \n",
      "Word: union     Probablity: 0.06896551724137931\n",
      "Word: nation     Probablity: 0.05172413793103448\n",
      "Word: war     Probablity: 0.05172413793103448\n",
      "Word: other     Probablity: 0.034482758620689655\n",
      "Word: cause     Probablity: 0.034482758620689655\n",
      "Word: conflict     Probablity: 0.034482758620689655\n",
      "Word: same     Probablity: 0.034482758620689655\n",
      "Word: offense     Probablity: 0.034482758620689655\n",
      "Word: right     Probablity: 0.034482758620689655\n",
      "Word: oath     Probablity: 0.017241379310344827\n",
      "Word: presidential     Probablity: 0.017241379310344827\n",
      "Word: first     Probablity: 0.017241379310344827\n",
      "Word: expiration     Probablity: 0.017241379310344827\n",
      "Word: great     Probablity: 0.017241379310344827\n",
      "Word: attention     Probablity: 0.017241379310344827\n",
      "Word: energies     Probablity: 0.017241379310344827\n",
      "Word: progress     Probablity: 0.017241379310344827\n",
      "Word: public     Probablity: 0.017241379310344827\n",
      "Word: future     Probablity: 0.017241379310344827\n",
      "Word: occasion     Probablity: 0.017241379310344827\n",
      "Word: inaugural     Probablity: 0.017241379310344827\n",
      "Word: city     Probablity: 0.017241379310344827\n",
      "Word: whole     Probablity: 0.017241379310344827\n",
      "Word: southern     Probablity: 0.017241379310344827\n",
      "Word: object     Probablity: 0.017241379310344827\n",
      "Word: insurgents     Probablity: 0.017241379310344827\n",
      "Word: government     Probablity: 0.017241379310344827\n",
      "Word: territorial     Probablity: 0.017241379310344827\n",
      "Word: magnitude     Probablity: 0.017241379310344827\n",
      "Word: duration     Probablity: 0.017241379310344827\n",
      "Word: sweat     Probablity: 0.017241379310344827\n",
      "Word: prayers     Probablity: 0.017241379310344827\n",
      "Word: almighty     Probablity: 0.017241379310344827\n",
      "Word: world     Probablity: 0.017241379310344827\n",
      "Word: providence     Probablity: 0.017241379310344827\n",
      "Word: woe     Probablity: 0.017241379310344827\n",
      "Word: believers     Probablity: 0.017241379310344827\n",
      "Word: wealth     Probablity: 0.017241379310344827\n",
      "Word: bondsman     Probablity: 0.017241379310344827\n",
      "Word: lash     Probablity: 0.017241379310344827\n",
      "Word: sword     Probablity: 0.017241379310344827\n",
      "Word: judgments     Probablity: 0.017241379310344827\n",
      "Word: lord     Probablity: 0.017241379310344827\n",
      "Word: work     Probablity: 0.017241379310344827\n",
      "Word: battle     Probablity: 0.017241379310344827\n",
      "Exiting Program....\n"
     ]
    }
   ],
   "source": [
    "#Author: @Nusan Rana\n",
    "#Student ID: 918392558\n",
    "from audioop import reverse\n",
    "from queue import PriorityQueue\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "\"\"\"Input: A word\n",
    "\n",
    "Output : List of words that can follow the input word, and their corresponding probabilities \n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1) Build a bigram LM using the following two steps:\n",
    "\n",
    "1A) Use nltk to compile all the unique bigrams from the corpus you used for the previous assignment.  \n",
    "\n",
    "1B) Compute probability of each bigram using MLE (count(w1 w2)/count(w1)) \n",
    "\n",
    "2) Predict next word using the following steps:\n",
    "\n",
    "2A) Get an input word from user, inpW.\n",
    "\n",
    "2B) Use the bigram LM built in step 1 to find all the bigrams where the input word, inpW, is w1.  Display all possible next words from these bigrams and their corresponding probabilities.  (Sort in descending order on probabilities)\n",
    "\n",
    "\"\"\"\n",
    "class bigram:\n",
    "\n",
    "    #function to normalize and tokenize corpus\n",
    "    #deletes all the punctuations\n",
    "    #takes nltk corpus as param, returns list of words\n",
    "    def corpus_tokenize(valuec):\n",
    "        corpus = ' '.join(valuec)\n",
    "        normalize_corpus = corpus.lower()\n",
    "        tokenize = word_tokenize(normalize_corpus)\n",
    "        tokenize=[word.lower() for word in tokenize if word.isalpha()]\n",
    "\n",
    "        return tokenize\n",
    "  \n",
    "    #counting frequency of all words in the corpus\n",
    "    #returns dict of word:freq\n",
    "    def vocab_builder(corpus):\n",
    "        vocab = {}\n",
    "        for x in corpus:\n",
    "            if vocab.get(x) == None:\n",
    "                vocab[x] = 1\n",
    "            else:\n",
    "                vocab[x] = vocab.get(x)+1\n",
    "        #print(vocab)\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    \n",
    "    #finding frequency of each bigram\n",
    "    #takes list of bigram as param, returns dict of bigram as key and frequency as value\n",
    "    def bigram_freq(bigrams):\n",
    "        bigram_freq  = {}\n",
    "        for x in bigrams:\n",
    "            if bigram_freq.get(x) == None:\n",
    "                    bigram_freq[x] = 1\n",
    "            else:\n",
    "                    bigram_freq[x] = bigram_freq.get(x)+1\n",
    "            #print(vocab)\n",
    "        return bigram_freq\n",
    "    \n",
    "    #1B) Compute probability of each bigram using MLE (count(w1 w2)/count(w1)) \n",
    "    def bigram_prob(bigram_freq, vocab):\n",
    "        bigram_prob = {}\n",
    "        for x in bigram_freq:\n",
    "            bigram_prob[x] = bigram_freq.get(x)/vocab.get(x[0]) #calculating probablity of each bigram and storing in dicta as corresponding value\n",
    "            \n",
    "        return bigram_prob\n",
    "    \n",
    "    \n",
    "    #2A) Get an input word from user, inpW.\n",
    "    #takes dict of bigram and prob, unique vocab as param\n",
    "    #prints all next word and prob in descending order\n",
    "    def user_input(bigram_probablity, vocab):\n",
    "        inpW = input(\"Enter a word: \")\n",
    "        user_word = inpW.lower() #normalizing user input\n",
    "        next_word = {} #hashmap to store next word and prob\n",
    "        while user_word != 'q': #loop until user wants to exit\n",
    "            if user_word in vocab: #checking is user input is vaid\n",
    "                for x in bigram_probablity:\n",
    "                    if user_word == x[0]:\n",
    "                        next_word[x[1]] = bigram_probablity.get(x) #inserting word as keu, prob as value\n",
    "                \n",
    "                #sorting next word by probablity\n",
    "                sorted_distance = dict(sorted(next_word.items(), key=lambda item: item[1],reverse=True)) #sorting words by probablity\n",
    "                \n",
    "                #printing all possible words and prob\n",
    "                print(f\"\\nPossible next words and probablity after \\\"{inpW}\\\": \")\n",
    "                for y in sorted_distance: #looping through possible words\n",
    "                    print(f\"Word: {y}     Probablity: {sorted_distance.get(y)}\")\n",
    "                    \n",
    "                sorted_distance.clear() #clearing dict for nextinput\n",
    "                next_word.clear() #clearing dict for next input\n",
    "                inpW = input(\"Enter a word: \")\n",
    "                user_word = inpW.lower()\n",
    "            else: #if user input is not in our vocab\n",
    "                print(f\"\\n\\\"{inpW}\\\" is not in corpus..\")\n",
    "                inpW = input(\"Enter a word: \")\n",
    "                user_word = inpW.lower()\n",
    "        print(\"Exiting Program....\")    \n",
    "    \n",
    "def run():  \n",
    "    #creating bigram class instance\n",
    "    bigram_obj = bigram\n",
    "    \n",
    "    inaugural_corpus = inaugural.words('1865-Lincoln.txt') #using inaugural corpus\n",
    "    tokened_corpus = bigram_obj.corpus_tokenize(inaugural_corpus) #tokenizing corpus\n",
    "    vocab_freq =  bigram_obj.vocab_builder(tokened_corpus)     #creating a dict that holds word and it's frequency\n",
    "    \n",
    "    #finding all the bigrams using bigrams function\n",
    "    bigram_list = list((bigrams(tokened_corpus))) #list of all the bigrams in the corpus_tokenize\n",
    "\n",
    "    #creating a dict and calling function to find it's frequency\n",
    "    bigram_freq =  bigram_obj.bigram_freq(bigram_list)\n",
    "    \n",
    "    #dict of bigram and probablity, calling function to find it's probablity\n",
    "    bigram_prob = bigram_obj.bigram_prob(bigram_freq, vocab_freq)\n",
    "    \n",
    "    #prompts for user\n",
    "    print(\"Total words in corpus:\", len(inaugural_corpus))\n",
    "    print(\"Unique vocab #: \",len(vocab_freq))\n",
    "    print(\"Total bigrams: \", len(bigram_list))\n",
    "    print(\"Unique bigram #: \",len(bigram_freq))\n",
    "    print(\"\\n\")\n",
    "    #calling function to get user input\n",
    "    user_word = bigram_obj.user_input(bigram_prob, vocab_freq)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Enter \\\"q\\\" to end: \")\n",
    "    print(\"Courpus: 1865-Lincoln.txt from inaugural corpus\")\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
