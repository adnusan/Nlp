{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\nusan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing inaugural corpus from nltk and using as document\n",
    "#query string\n",
    "\n",
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "doc1 = \" \".join(inaugural.words('1865-Lincoln.txt'))\n",
    "doc2 = \" \".join(inaugural.words('1933-Roosevelt.txt'))\n",
    "doc3 = \" \".join(inaugural.words('2009-Obama.txt'))\n",
    "\n",
    "query = \"life learning\"\n",
    "\n",
    "\n",
    "#print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document  achieve  could  ascribe  astounding  just  impending  read  \\\n",
      "0  Term Frequency        1      2        1           1     2          1     1   \n",
      "\n",
      "   knew  Countrymen  ...  shall  is  whole  gives  by  nations  ?  a  wealth  \\\n",
      "0     1           1  ...      5   6      1      2   6        1  1  7       1   \n",
      "\n",
      "   wounds  \n",
      "0       1  \n",
      "\n",
      "[1 rows x 364 columns]\n",
      "         Document  emergency  yours  grim  situation  tragedy  policy  rights  \\\n",
      "0  Term Frequency          4      1     1          1        1       3       1   \n",
      "\n",
      "   products  Congress  ...  selfish  purchase  by  candor  Hand  ways   a  \\\n",
      "0         1         4  ...        1         1  19       1     1     1  38   \n",
      "\n",
      "   wealth  can  midst  \n",
      "0       1   11      1  \n",
      "\n",
      "[1 rows x 747 columns]\n",
      "         Document  humble  achieve  celebrated  rights  fist  much  hard  \\\n",
      "0  Term Frequency       1        1           1       1     1     1     4   \n",
      "\n",
      "   faithful  free  ...  sacrifices  energy  ways   a  wealth  midst  can  far  \\\n",
      "0         1     2  ...           1       1     1  47       2      1   13    4   \n",
      "\n",
      "   spoken  Domestic  \n",
      "0       1         1  \n",
      "\n",
      "[1 rows x 939 columns]\n"
     ]
    }
   ],
   "source": [
    "#finding term frequency from documents\n",
    "#term -frequenvy :word occurences in a document\n",
    "\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list: \n",
    "        doc1_lst = doc.split(\" \")\n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
    "\n",
    "        for token in doc1_lst:\n",
    "            wordDict_1[token] +=  1\n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    #adding new column for term frequency count\n",
    "        df.insert(loc=idx, column='Document', value=new_col) #adding new column for document name\n",
    "        print(df)\n",
    "        \n",
    "compute_tf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document   achieve     could   ascribe  astounding      just  \\\n",
      "0  Normalized TF  0.001274  0.002548  0.001274    0.001274  0.002548   \n",
      "\n",
      "   impending      read      knew  Countrymen  ...     shall        is  \\\n",
      "0   0.001274  0.001274  0.001274    0.001274  ...  0.006369  0.007643   \n",
      "\n",
      "      whole     gives        by   nations         ?         a    wealth  \\\n",
      "0  0.001274  0.002548  0.007643  0.001274  0.001274  0.008917  0.001274   \n",
      "\n",
      "     wounds  \n",
      "0  0.001274  \n",
      "\n",
      "[1 rows x 364 columns]\n",
      "        Document  emergency     yours      grim  situation   tragedy  \\\n",
      "0  Normalized TF   0.001939  0.000485  0.000485   0.000485  0.000485   \n",
      "\n",
      "     policy    rights  products  Congress  ...   selfish  purchase       by  \\\n",
      "0  0.001454  0.000485  0.000485  0.001939  ...  0.000485  0.000485  0.00921   \n",
      "\n",
      "     candor      Hand      ways        a    wealth       can     midst  \n",
      "0  0.000485  0.001939  0.000485  0.01842  0.000485  0.005332  0.000485  \n",
      "\n",
      "[1 rows x 747 columns]\n",
      "        Document    humble   achieve  celebrated    rights      fist  \\\n",
      "0  Normalized TF  0.000367  0.000367    0.000367  0.000367  0.000367   \n",
      "\n",
      "       much      hard  faithful      free  ...  sacrifices    energy  \\\n",
      "0  0.000367  0.001467  0.000367  0.000734  ...    0.000367  0.000367   \n",
      "\n",
      "       ways         a    wealth     midst       can       far    spoken  \\\n",
      "0  0.000367  0.017241  0.000734  0.000367  0.004769  0.001467  0.000367   \n",
      "\n",
      "   Domestic  \n",
      "0  0.000367  \n",
      "\n",
      "[1 rows x 939 columns]\n"
     ]
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split() #normalizing using lowering the words\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt)\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    #adding new col for normalized term frequency count\n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "documents = [doc1, doc2, doc3]\n",
    "\n",
    "tf_doc = compute_normalizedtf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.5, 'learning': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Normalized TF for the query string(\"life learning\")\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "\"\"\"\n",
    "Example : Dot roduct(Query, Document1) \n",
    "\n",
    "     life:\n",
    "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n",
    "     sqrt(tfidf(life w.r.t query)) * \n",
    "     sqrt(tfidf(life w.r.t doc1))\n",
    "     \n",
    "     learning:\n",
    "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
    "     sqrt(tfidf(learning w.r.t query)) * \n",
    "     sqrt(tfidf(learning w.r.t doc1))\n",
    "\n",
    "\"\"\"\n",
    "#Create a variation of Program_1 that uses only TF representation to compute the similarity between a query and document.  \n",
    "def cosine_similarity(tf_doc, query, doc_num):\n",
    "     dot_product = 0\n",
    "     qry_mod = 0\n",
    "     doc_mod = 0\n",
    "     tokens = query.split()\n",
    "     \n",
    "     for keyword in tokens:\n",
    "          dot_product += query_norm_tf[keyword] * tf_doc[keyword][tf_doc['Document'] == doc_num]\n",
    "          qry_mod += (query_norm_tf[keyword] * query_norm_tf[keyword])\n",
    "          doc_mod += (tf_doc[keyword][tf_doc['Document'] == doc_num] * tf_doc[keyword][tf_doc['Document'] == doc_num])\n",
    "     qry_mod = math.sqrt(qry_mod)\n",
    "     doc_mod = math.sqrt(doc_mod)\n",
    "     \n",
    "     #implement formula\n",
    "     denominator = qry_mod * doc_mod\n",
    "     cos_sim = dot_product / denominator\n",
    "     return cos_sim\n",
    "\n",
    "\n",
    "from collections import Iterable\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [72], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m         cos_sim\u001b[39m.\u001b[39mappend(cosine_similarity(tf_doc, query, doc_num)\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m cos_sim\n\u001b[1;32m----> 6\u001b[0m similarity_docs \u001b[39m=\u001b[39m rank_similarity_docs(tf_doc)\n\u001b[0;32m      7\u001b[0m doc_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m1865-Lincoln.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1933-Roosevelt.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2009-Obama.txt\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity between query and documents\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [72], line 4\u001b[0m, in \u001b[0;36mrank_similarity_docs\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m cos_sim \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m doc_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[1;32m----> 4\u001b[0m     cos_sim\u001b[39m.\u001b[39mappend(cosine_similarity(tf_doc, query, doc_num)\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m cos_sim\n",
      "Cell \u001b[1;32mIn [69], line 25\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(tf_doc, query, doc_num)\u001b[0m\n\u001b[0;32m     22\u001b[0m tokens \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m keyword \u001b[39min\u001b[39;00m tokens:\n\u001b[1;32m---> 25\u001b[0m      dot_product \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m query_norm_tf[keyword] \u001b[39m*\u001b[39m tf_doc[keyword][tf_doc[\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m doc_num]\n\u001b[0;32m     26\u001b[0m      qry_mod \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (query_norm_tf[keyword] \u001b[39m*\u001b[39m query_norm_tf[keyword])\n\u001b[0;32m     27\u001b[0m      doc_mod \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (tf_doc[keyword][tf_doc[\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m doc_num] \u001b[39m*\u001b[39m tf_doc[keyword][tf_doc[\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m doc_num])\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data):\n",
    "    cos_sim = []\n",
    "    for doc_num in range(len(data)):\n",
    "        cos_sim.append(cosine_similarity(tf_doc, query, doc_num).tolist())\n",
    "    return cos_sim\n",
    "similarity_docs = rank_similarity_docs(tf_doc)\n",
    "doc_names = [\"1865-Lincoln.txt\", \"1933-Roosevelt.txt\", \"2009-Obama.txt\"]\n",
    "print(\"Similarity between query and documents\")\n",
    "print(doc_names)\n",
    "print(list(flatten(similarity_docs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6801ae3e431dd180d2cc72d84ce6feb33b452596a28ade9b15354c20274f8a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
