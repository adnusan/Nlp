{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents\n",
    "doc1 = \"I want to start learning to charge something in life\"\n",
    "doc2 = \"reading something about life no one else knows\"\n",
    "doc3 = \"Never stop learning\"\n",
    "#query string\n",
    "query = \"life learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document  want  in  life  learning  start  to  charge  something  I\n",
      "0  Term Frequency     1   1     1         1      1   2       1          1  1\n",
      "         Document  one  no  life  about  else  reading  something  knows\n",
      "0  Term Frequency    1   1     1      1     1        1          1      1\n",
      "         Document  Never  learning  stop\n",
      "0  Term Frequency      1         1     1\n"
     ]
    }
   ],
   "source": [
    "#finding term frequency from documents\n",
    "#term -frequenvy :word occurences in a document\n",
    "\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list: \n",
    "        doc1_lst = doc.split(\" \")\n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
    "\n",
    "        for token in doc1_lst:\n",
    "            wordDict_1[token] +=  1\n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    #adding new column for term frequency count\n",
    "        df.insert(loc=idx, column='Document', value=new_col) #adding new column for document name\n",
    "        print(df)\n",
    "        \n",
    "compute_tf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document  want   in  life  learning  start   to  charge  something  \\\n",
      "0  Normalized TF   0.1  0.1   0.1       0.1    0.1  0.2     0.1        0.1   \n",
      "\n",
      "     I  \n",
      "0  0.1  \n",
      "        Document    one     no   life  about   else  reading  something  knows\n",
      "0  Normalized TF  0.125  0.125  0.125  0.125  0.125    0.125      0.125  0.125\n",
      "        Document     Never  learning      stop\n",
      "0  Normalized TF  0.333333  0.333333  0.333333\n"
     ]
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split() #normalizing using lowering the words\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt)\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    #adding new col for normalized term frequency count\n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "\n",
    "tf_doc = compute_normalizedtf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2.09861228866811,\n",
       " 'want': 2.09861228866811,\n",
       " 'to': 2.09861228866811,\n",
       " 'start': 2.09861228866811,\n",
       " 'learning': 1.4054651081081644,\n",
       " 'charge': 2.09861228866811,\n",
       " 'something': 1.4054651081081644,\n",
       " 'in': 2.09861228866811,\n",
       " 'life': 1.4054651081081644,\n",
       " 'reading': 2.09861228866811,\n",
       " 'about': 2.09861228866811,\n",
       " 'no': 2.09861228866811,\n",
       " 'one': 2.09861228866811,\n",
       " 'else': 2.09861228866811,\n",
       " 'knows': 2.09861228866811,\n",
       " 'Never': 2.09861228866811,\n",
       " 'stop': 2.09861228866811}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inverse Document Frequency\n",
    "def inverseDocumentFrequency(term, allDocuments): #term is the word and allDocuments is the list of documents\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    #calculating idf for each word in the documents\n",
    "    for doc in range (0, len(allDocuments)):\n",
    "        if term.lower() in allDocuments[doc].lower().split(): #normalizing the term\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0: #if the term is not in the document, then the idf is 0\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def compute_idf(documents): #documents is the list of documents\n",
    "    idf_dict = {} #dictionary to store the idf values\n",
    "    for doc in documents:\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents) #calling the inverseDocumentFrequency function\n",
    "    return idf_dict\n",
    "idf_dict = compute_idf([doc1, doc2, doc3])\n",
    "\n",
    "compute_idf([doc1, doc2, doc3]) #computing idf for each word in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc      life  learning\n",
      "0  0.0  0.140547  0.140547\n",
      "1  1.0  0.175683  0.000000\n",
      "2  2.0  0.000000  0.468488\n"
     ]
    }
   ],
   "source": [
    "#calculating tf*idf\n",
    "# tf-idf score across all docs for the query string(\"life learning\")\n",
    "def compute_tfidf_with_alldocs(documents , query): #documents is the list of documents and query is the query string\n",
    "    tf_idf = [] #list  to store the tf-idf values\n",
    "    index = 0\n",
    "    query_tokens = query.split()\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = tf_doc[index]\n",
    "        sentence = doc.split()\n",
    "        for word in sentence: #calculating tf-idf for each word in the documents\n",
    "            for text in query_tokens: #calculating tf-idf for each word in the query string\n",
    "                if(text == word): #if the word in the query string is in the document, then calculate tf-idf\n",
    "                    idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1\n",
    "    df.fillna(0 , axis=1, inplace=True)\n",
    "    return tf_idf , df\n",
    "            \n",
    "documents = [doc1, doc2, doc3] #documents\n",
    "tf_idf , df = compute_tfidf_with_alldocs(documents , query) #calling the compute_tfidf_with_alldocs function\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.5, 'learning': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Normalized TF for the query string\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 1.4054651081081644, 'learning': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "#idf score for the query string\n",
    "def compute_query_idf(query): #computes the idf score for the query string\n",
    "    idf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    documents = [doc1, doc2, doc3]\n",
    "    for word in sentence:\n",
    "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
    "    return idf_dict_qry\n",
    "idf_dict_qry = compute_query_idf(query)\n",
    "print(idf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.7027325540540822, 'learning': 0.7027325540540822}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf score for the query string\n",
    "def compute_query_tfidf(query): #this function calculates the tf-idf score for the query string\n",
    "    tfidf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    for word in sentence:\n",
    "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
    "    return tfidf_dict_qry\n",
    "tfidf_dict_qry = compute_query_tfidf(query)\n",
    "print(tfidf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nusan\\AppData\\Local\\Temp\\ipykernel_17856\\2873275391.py:37: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "\"\"\"\n",
    "Example : Dot roduct(Query, Document1) \n",
    "\n",
    "     life:\n",
    "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n",
    "     sqrt(tfidf(life w.r.t query)) * \n",
    "     sqrt(tfidf(life w.r.t doc1))\n",
    "     \n",
    "     learning:\n",
    "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
    "     sqrt(tfidf(learning w.r.t query)) * \n",
    "     sqrt(tfidf(learning w.r.t doc1))\n",
    "\n",
    "\"\"\"\n",
    "#this function calculates the cosine similarity between the query string and the documents\n",
    "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
    "    dot_product = 0\n",
    "    qry_mod = 0\n",
    "    doc_mod = 0\n",
    "    tokens = query.split()\n",
    "   \n",
    "    for keyword in tokens:\n",
    "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n",
    "        #||Query||\n",
    "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
    "        #||Document||\n",
    "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
    "    qry_mod = np.sqrt(qry_mod)\n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    #implement formula\n",
    "    denominator = qry_mod * doc_mod\n",
    "    cos_sim = dot_product/denominator\n",
    "     \n",
    "    return cos_sim\n",
    "\n",
    "from collections import Iterable\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document1', 'Document2', 'Document3']\n",
      "[1.0, 0.7071067811865475, 0.7071067811865475]\n"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data): #data is a dataframe\n",
    "    cos_sim =[] # to store the cosine similarity values\n",
    "    for doc_num in range(0 , len(data)):\n",
    "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist()) #calling the cosine_similarity function to store the cosine similarity values\n",
    "    return cos_sim\n",
    "similarity_docs = rank_similarity_docs(documents) #calling the rank_similarity_docs function\n",
    "doc_names = [\"Document1\", \"Document2\", \"Document3\"] #document names\n",
    "print(doc_names) \n",
    "print(list(flatten(similarity_docs))) #calling the flatten function to flatten the list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6801ae3e431dd180d2cc72d84ce6feb33b452596a28ade9b15354c20274f8a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
