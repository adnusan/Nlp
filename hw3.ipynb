{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/nusanrana/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Total unique words:  9304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"HA #3\n",
    "Simple Word Autocomplete / Autocorrect and Word Probability\n",
    "\n",
    "Input argument: A string of characters, XYZ (it doesn't have to be a complete word)\n",
    "\n",
    "Output: \n",
    "\n",
    "If the input string exists in your program's vocabulary then your program should output \"XYZ is a complete and correct word in English.\"\n",
    "\n",
    "If the input string doesn't exist in your program's vocabulary then your program should output the 5 closest words to it as measured by Levenshtein distance, and their probability.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1) Build a vocabulary (set of all unique words) using any English corpus from nltk. This is your program's vocabulary.  (Choose a small corpus so that the vocabulary isn't too big.  This will help with step 4.b.i below.)\n",
    "\n",
    "2) Find the number of occurrences (frequency) of each word in the vocabulary.  Also, find the total number of words in the chosen corpus (N).\n",
    "\n",
    "3) Find the relative frequency of each word W where relative frequency of W = frequency_of_W / N. This relative frequency is the probability (likelihood) of each word in the corpus.  \n",
    "\n",
    "4) For every input string XYZ:\n",
    "\n",
    "4.a) If the input string XYZ exists in your vocabulary, return \"XYZ is a complete and correct word as per corpus ___, and its probability is __\"\n",
    "\n",
    "4.b) If the input string doesn't exist in your vocabulary, perform the below steps:\n",
    "\n",
    "4.b.i) Calculate the similarity between each word in the vocabulary and the input string using Levenshtein distance. (Use any open-source python library for calculating Levenshtein distance.) \n",
    "\n",
    "4.b.ii) Output the closest top 5 words as per Levenshtein distance to the input string. Also, output the probability for each of the 5 words.\n",
    "\n",
    "Submit to ilearn\n",
    "1. Your program or python notebook.\n",
    "2. Log of output showing examples of the two different user input scenarios (4.a & 4.b).\"\"\"\n",
    "\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class hw:\n",
    "   #total_words_in_corpus = len(inaugural_corpus)\n",
    "   #print(\"Total # of words in corpus: \", total_words_in_corpus)\n",
    "   #function to normalize and tokenize corpus\n",
    "   #takes nltk corpus as param, returns list of words\n",
    "   def corpus_tokenize(cor):\n",
    "      corpus = ' '.join(cor)\n",
    "      normalize_corpus = corpus.lower()\n",
    "      tokenize = word_tokenize(normalize_corpus)\n",
    "      return tokenize\n",
    "\n",
    "\n",
    "   #function to build vocab list\n",
    "   # this function takes list of words and put them in hash map\n",
    "   # key = words, value = frequency of word\n",
    "   #returns dict of words : frequency\n",
    "   def vocab_builder(tokenize_vocab):\n",
    "      vocab = {}\n",
    "      for x in tokenize_vocab:\n",
    "         if vocab.get(x) == None:\n",
    "            vocab[x] = 1\n",
    "         else:\n",
    "            vocab[x] = vocab.get(x)+1\n",
    "      return vocab\n",
    "   \n",
    "   def find_word():\n",
    "      return 0\n",
    "\n",
    "\n",
    "   \n",
    "   #print(unique_vocab)\n",
    "\n",
    "   #print(\"Total unique words: \",len(vocabulary))\n",
    "   #print(vocabulary)\n",
    "inaugural_corpus = inaugural.words()\n",
    "testing = hw\n",
    "tokened_words = testing.corpus_tokenize(inaugural_corpus) #getting tokenized and mormalized list of words from corpus\n",
    "vocabulary = testing.vocab_builder(tokened_words) #making a dist that has word as key and frequency as value \n",
    "\n",
    "\n",
    "def demo():\n",
    "   print(\"test\")\n",
    "   print(\"Total unique words: \",len(vocabulary))\n",
    "   user_input = input(\"Enter word\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   demo()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36713bed272a9d114316a294fb6c1a826f423acbc1d9527c9480bc3366607df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
